
[{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"\rItem Summary #\rWith this, the 日本語 learner can switch between Kanji and Hiragana seamlessly, allowing for an uninterrupted reading experience.\nItem Description #\rWhen reading Japanese as a new learner, the amount of new, unknown Kanji can become overwhelming, and can make people feel frustrated with having to go away from the screen to look up words. With this extension, learners can easily switch between Kanji and Hiragana to ensure an uninterrupted reading experience. They can also toggle specific words to switch back and forth for Hiragana/Kanji. They can also simply press the button again to switch back between the two.\nUsage #\rLet\u0026rsquo;s take a page from the Aozora Bunko, for example. You can notice all of the various kanji on the screen that a new learner might not know: Now, clicking on the extension will give us an option to switch between kanji and hiragana: After clicking on the extension, we can see that all of the kanji have been transformed to hiragana, and the kanji that have been transformed are underlined so the user knows which hiragana belong to which word:\rLet\u0026rsquo;s click back on the very first word, to see what its original kanji was! You\u0026rsquo;ll be able to click on this multiple times to switch back and forth: Awesome! We can see that 蜘蛛 is くも in hiragana! Using some reading in context, maybe you could have found out that it means spider. Now, what if we want to change it back? Clicking the extension resets everything to how it was: Motivation #\rI didn\u0026rsquo;t like having my reading experience interrupted, so I built this chrome extension. I hope you like the extension! Let me know your thoughts!\n","date":"24 July 2024","externalUrl":null,"permalink":"/posts/kanji-to-hiragana/kanji-to-hiragana/","section":"Posts","summary":"Item Summary #\rWith this, the 日本語 learner can switch between Kanji and Hiragana seamlessly, allowing for an uninterrupted reading experience.","title":"Kanji to Hiragana","type":"posts"},{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/","section":"Kushal Vajrala","summary":"","title":"Kushal Vajrala","type":"page"},{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/authors/me/","section":"Authors","summary":"","title":"Me","type":"authors"},{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/tags/swe/","section":"Tags","summary":"","title":"SWE","type":"tags"},{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/tags/%E6%97%A5%E6%9C%AC%E8%AA%9E/","section":"Tags","summary":"","title":"日本語","type":"tags"},{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/tags/ai/ml/","section":"Tags","summary":"","title":"AI/ML","type":"tags"},{"content":"\ror, how my love for video games and AI/ML brought me to an intersection of them with reinforcement learning.\nThe information in this post is not intended to be a thorough mathematical primer on the topic. Rather, it is simply to showcase and explain a cool concept I\u0026rsquo;ve learned and implemented myself, and to explain it to a layman. I will be discussing some math here and there, but I recommend the Reinforcement Learning book by Sutton and Barto if you\u0026rsquo;re really looking to get your hands dirty.\nIntroduction #\rA common prank my friends and I like to conduct is to bring people to a game of Mao. Essentially, the uninitiated are not allowed to know the rules of this card game. They must figure it out by watching other people, and they usually end up suffering in-game penalties frequently as a result.\nHow would you approach this game (or, for the initiated, how did you first approach this game)? I bet that when starting the game, you would do a random move and see where that gets you. You would observe when you get penalized for something, and try to associate that pattern so that you try not to let it happen again. You might guess wrong, and then try a slightly tweaked version of your move to see if it happens again. Similarly with video games, I do the same thing: see what works, see what doesn\u0026rsquo;t, and try to keep doing what works and what doesn\u0026rsquo;t work. Lord knows how many times that I, someone who\u0026rsquo;s never played DnD before, tried to adjust my Baldur\u0026rsquo;s Gate 3 classes, actions, and story paths to get the most satisfying one across multiple playthroughs.\nDefinition #\rThis, my friends, is the essence of reinforcement learning: how an agent conducts actions in an environment to maximize its overall reward. We\u0026rsquo;ll get into what these terms mean in a second.\nA very common image that gets utilized when describing RL is as follows: If you recognize what shape this takes, you are correct in assuming that this is a Markov decision process. If you don\u0026rsquo;t, no worries.\nLet\u0026rsquo;s break this picture down piece by piece. You have an agent (a model, a piece of code, etc.) that takes an action and interacts with its environment. Then, the agent is given two pieces of input from the interpreter (a fancy way of saying something that translates the outcomes of the action to the environment). The state, which you can think of a snapshot of the environment after the action, and the reward, which is an incentive to the model. For an RL algo trading stocks, the reward in simple terms could be its profits. For checkers, the number of pieces captured. In the case of Mao, the reward would be\u0026hellip; well. Wouldn\u0026rsquo;t you like to know? We then loop this process, and keep learning.\nNote: the results of the prior action does not impact the probability of choosing the current action. Just a bit of a disclaimer before we move forward.\nBy this point, I\u0026rsquo;m sure you\u0026rsquo;ve been understanding everything I\u0026rsquo;ve said; however, one question probably still remains. \u0026ldquo;How?\u0026rdquo; And that, my friends, is what the next section is for. We\u0026rsquo;ll discuss an algorithm that I implemented!\nQ-Learning #\rLots of people I know feel like this whenever they encounter/encountered a math test: The Algo #\rDon\u0026rsquo;t worry! I will do my best to walk you through the math. (If you weren\u0026rsquo;t worrying at all, then that\u0026rsquo;s good) Here\u0026rsquo;s the equation for the algorithm we\u0026rsquo;re going to cover today, Q-learning:\n$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]$$\nYou see the \\(Q(S_t, A_t)\\). The \\(Q\\) is a function that we call a Q table. This is defined as follows: $$Q(s, a) = \\mathbb{E}[R_t \\mid S_t = s, A_t = a]$$\nwhere the inputs to the Q table are the state s and action a that the agent takes. The \\(\\mathbb{E}\\) refers to the expected value, \\(R_t\\) refers to the total reward up to the timestep (basically, what point of time we are in) t, and \\(S_t\\) and \\(A_t\\) are the state and action at timestep t. What does this mean? Basically: the Q table is keeping track of the cumulative reward up to time t for every (state, action) pair. Think about it: it\u0026rsquo;s basically what you do in a game you don\u0026rsquo;t know, right? You try something (action) in a certain situation (state) and see what that does for you (reward/no reward/sometimes negative reward). The Q table is a convenient way of keeping track of this.\nNow, you\u0026rsquo;ll probably have seen already that the original Q learning algorithm updates itself. That\u0026rsquo;s cool, right? That\u0026rsquo;s how this model learns. Now that you have an understanding of the Q table, let\u0026rsquo;s go into the rest of the equation.\nAs we already covered, \\(Q(S_t, A_t)\\) is basically the current estimate of the reward for the (state, action) pair at time t. What about the \\(\\left[R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]\\)? \\(R_{t+1}\\) is essentially the reward after the action was done at timestep t, and the \\(\\gamma\\) is the discount factor (we will get into this in a bit). The \\(\\max_a Q(S_{t+1}, a)\\) is giving us the maximum possible reward for the next state at timestep t+1 after the corresponding action was done at time t; I know that was a mouthful, but basically, we\u0026rsquo;re figuring out which action a at timestep t gives us the best reward for the next state, and we\u0026rsquo;re rolling with that. The sum of these two terms is the target value. Our reward at this spot, plus the discounted future reward given that we take that action.\nIf you\u0026rsquo;ve taken a finance class before, you\u0026rsquo;ve probably learned about the time value of money. Essentially, $100 today is worth more than $100 next year. Why? The simple answer is opportunity cost. However, if you aren\u0026rsquo;t satisfied with that answer, or desire a more technical proof/explanation, feel free to look for one. We apply this opportunity cost with \\(\\gamma\\), our discount factor. We discount future rewards, since the present rewards are worth much more, hence the multiplication. As you can guess from the explanation, the discount factor is usually between 0 and 1, though closer to 1. We don\u0026rsquo;t want to discount it TOO much.\nSubtracting the original \\(Q(S_t, A_t)\\) gives us something called the *temporal difference (TD) error. Basically, we\u0026rsquo;re seeing how far away our current Q value in the table is to the target value (which is the sum of the reward and the discounted max possible future reward that we discussed earlier).\nWe then multiply this whole thing by the \\(\\alpha\\), the learning rate. We don\u0026rsquo;t want to immediately swap out our Q table values to this one! We want to gradually make our way there, because we have no idea if what we\u0026rsquo;ve learned is exactly true or not. Similar to our Mao example: you don\u0026rsquo;t know if the reason you didn\u0026rsquo;t get penalized is completely due to you guessing the right rule. You might have gotten lucky! So you remember it, and keep an eye out for it next time, adjusting your expectations accordingly. You\u0026rsquo;ve by now deduced that the learning rate is a value between 0 and 1, but closer to 0, since you don\u0026rsquo;t want to update your values too quickly.\nNow, for the final part. You adjust the \\(Q(S_t, A_t)\\) by adding what was in there previously with the learning rate times the TD error! This gives our Q value a little nudge in the right direction.\nImplementation Explanation #\rBut \u0026ldquo;wait!\u0026rdquo;, you might say. How do we even start? Looking at the diagram earlier, it looked like a never ending loop. When do we start, and when do we stop? How do we make Q values in the first place? Let\u0026rsquo;s get into that too.\nLet\u0026rsquo;s answer the easier questions first.\nWhen we start this, we initialize our current state to 0s (or nulls, depdending on how you would like to implement this), and we initialize our Q table to have all 0s as well. Starting and stopping involves two things: number of episodes, and the number of time steps per episode. What\u0026rsquo;s happening here is that each episode is the number of times we let the agent explore the environment (or, more simply, play the game) and the number of time steps is essentially the total amount of time we give the agent per episode. Sure, that\u0026rsquo;s all cool. But what happens if the Q table contains all zeroes? How do we even start populating it with values? That\u0026rsquo;s a good question. We\u0026rsquo;re going to get into something called an epsilon (\\(\\epsilon\\)) greedy strategy.\n\\(\\epsilon\\) Greedy Strategy #\rLet\u0026rsquo;s go back to the initial discussion of the game of Mao. When you start playing the game, you don\u0026rsquo;t know anything except what your options are, which are to essentially put a card from your hand down. That\u0026rsquo;s all you know. From there, you randomly experiment and try to see what works and what doesn\u0026rsquo;t work. After you\u0026rsquo;ve gotten a hang of the game, however, you don\u0026rsquo;t want to keep doing random stuff: you want to go ahead and try to use what you know to win. This is known as exploration/exploitation.\nWhen our agent first starts interacting with the environment, let\u0026rsquo;s give it some freedom to just randomly explore and try things and see what that gives it. However, as the agent gets a hang of things as it plays through a good number of episodes, we want it to exploit what it has learned in its Q table for the max possible reward.\nThis can be done through any sort of setup. It\u0026rsquo;s really up to you regarding how you wish to set this up, as long as it achieves the goal of decreasing your \\(\\epsilon\\) (exploration/exploitation coefficient) over the episodes so the agent can first explore, then exploit. Here is my implementation: $$\\epsilon \\leftarrow \\epsilon_{\\text{min}} + (\\epsilon_{\\text{max}} - \\epsilon_{\\text{min}}) \\cdot e^{(-d\\cdot p)}$$\nWhere p is the episode number, \\(\\epsilon\\) and \\(\\epsilon_{\\text{max}}\\) start off initialized as 1, and \\(\\epsilon_{\\text{min}}\\) and d (\\(\\epsilon\\) decay: the rate at which the \\(\\epsilon\\) value decreases) are usually initialized to values that are between 0 and 1, but usually much closer to 0. You can probably intuit that as the episodes go on, the \\(\\epsilon\\) value decreases. But how do we use this? Simple, really.\nEvery time step, we pick a random number between 0 and 1. If this number is greater than \\(\\epsilon\\), we just use whatever is in our Q table and make the decision from what we\u0026rsquo;ve seen. However, if it\u0026rsquo;s less than or equal to our \\(\\epsilon\\) value, then we just randomly pick an action and roll with it, and see what that gives us.\nOnce again, you\u0026rsquo;ll be able to intuit that the first case probably happens later on in the episodes, while the second case happens towards the beginning of the episodes. We\u0026rsquo;re done with the theory!\nMess w/My Code! #\rYou might not have a perfect grasp on the algorithm yet, especially if you haven\u0026rsquo;t recently been studying or doing math. That\u0026rsquo;s okay. Seeing it in code form will help you out there.\nWe\u0026rsquo;re going to be using Gymnasium to help us set up. We\u0026rsquo;re going to be using their Cart-Pole game. What\u0026rsquo;s happening here, and why did I choose this environment to do Q learning in?\nCart-pole is a famous intro for those who are dipping their feet into due to its simplicity. Basically, you are given a cart with a pendulum on top of it. The cart rolls on a frictionless surface. The action space (what all can the agent possibly do here?) is simple: push the cart left, and push the cart right. The objective is simple: keep the pendulum up for as long as possible. It\u0026rsquo;s considered a \u0026ldquo;fail\u0026rdquo; if the pendulum falls below a certain angle.\nHere\u0026rsquo;s my GitHub repo for my Q learning implementation of cart-pole. Read through the file, see if you can understand it and try to rewrite it yourself. I\u0026rsquo;ve included some comments explaining stuff, as well as links to the documentation if you want to understand it more. Don\u0026rsquo;t hesitate to leave a comment if there\u0026rsquo;s anything you\u0026rsquo;re confused about.\nInstallation/Prereqs #\rLast blog post was Hugo. This one is Python. Ensure that you have Python and pip installed, and then run the following commands. Note, this is assuming that you\u0026rsquo;re not using conda.\npip install gymnasium pip install numpy pip install matplotlib git clone https://github.com/vajralakushal/cart-pole.git Now, cd into the directory of the cart-pole and run:\npython cart-pole-q.py I\u0026rsquo;ve given you the option to select some paramters. Once you do, the model takes some time to train, and then a pyplot window should show up, plotting the average reward of the agent across every thousand episodes. Can you come up with a set of parameters where the cart-pole survives the longest (i.e. where the graph has the highest points).\nSources #\rHuge shoutout to the videos in this playlist. Heavily inspired me to write my own intro to this topic. Everything else is linked accordingly.\nThanks! #\rThanks for reading! Feel free to leave any comments if you have any!\n","date":"24 July 2024","externalUrl":null,"permalink":"/posts/basic-rl/cart-pole/","section":"Posts","summary":"or, how my love for video games and AI/ML brought me to an intersection of them with reinforcement learning.","title":"Cart Pole","type":"posts"},{"content":"","date":"24 July 2024","externalUrl":null,"permalink":"/tags/reinforcement-learning/","section":"Tags","summary":"","title":"Reinforcement-Learning","type":"tags"},{"content":"","date":"18 July 2024","externalUrl":null,"permalink":"/posts/how-to-japanese/how-to-japanese/","section":"Posts","summary":"","title":"","type":"posts"},{"content":"or, a first timer\u0026rsquo;s journey messing with static sites.\nWhat is Hugo? #\rI\u0026rsquo;ll just link them for you and save you the trouble of having to read my own paraphrasing. I can\u0026rsquo;t resist, however: it\u0026rsquo;s essentially an open source service that helps you generate static websites. Documentation, blogs, portfolios, you name it. I was inspired by Zoe\u0026rsquo;s blog and wanted to create my own.\nThe Goal #\rThis blog post is essentially a letter to my past self. If I had this blog post maybe a few hours earlier, I could\u0026rsquo;ve saved myself a lot of headache.\nInstallation #\rCurrently, I\u0026rsquo;m working out of my Windows PC, so I\u0026rsquo;ll go over this part assuming you are doing the same. If not, refer to this site.\nEnsure that Chocolatey, a windows package manager (similar to brew on MacOS or pacman on Arch. I do not use Arch btw). Same thing with Winget Funnily enough, same thing with Powershell 7 Run this command, but in Powershell 7: choco install hugo-extended Site Creation #\rYay! Everything has been installed and is ready to go. I use VS code just because the files are not as numerous to where I would need to use neovim or anything, but use whatever you\u0026rsquo;re comfortable with. Just make sure to use Powershell 7 as your shell here on out.\nCheck to see if Hugo was installed correctly with hugo version. At the time of this post, it should be around 0.129.0.\nHere comes the tricky part. To be fair, it was only tricky for me simply because I already had the makings of a barebones Next.js website, but I was tired of the learning curve. I\u0026rsquo;ll definitely come back to it one day, but Hugo just provided an easier option.\nCreate a repository on GitHub and name it \u0026ldquo;username.github.io\u0026rdquo;. (Does the period go inside the quotations or outside? It looks better inside but logically it feels better outside). Clone into this repo. Now, here\u0026rsquo;s where the fun begins.\nNavigate into the repo you just cloned and run this line:\nhugo new site example.com Now, we\u0026rsquo;ll do a bit of configuring here.\nmv \\example.com\\* .\rmv \\example.com\\.* # should be redundant, but eh why not\rrmdir \\example.com\\ If you look at your directory now, it should look like this\nGreat! Now, we can set up your hugo.toml as you wish. Add a title param in your hugo.toml file (anywhere inside works, as long as it\u0026rsquo;s not part of a param):\ntitle = Kushal Vajrala Explore the different configs you can use. From here on out, it\u0026rsquo;s just a matter of finding out your favorite theme: install and run for it!\nThe very adventurous will probably stop reading here, and go tinker on their own now that things are set up somewhat. However, I\u0026rsquo;m not totally sure that when I was working on this earlier. Let\u0026rsquo;s use the theme that I\u0026rsquo;m using for this tutorial, hugo-coder. I was able to set up my stuff pretty fast with this clean, minimalist theme.\nI would really rather not rewrite the download instructions they\u0026rsquo;ve so kindly given, so make sure you read and implement carefully. I recommend installing it as a git submodule.\nShowing off your literary masterpieces #\rNow, here\u0026rsquo;s where the fun (really, the writing) begins!\nAssuming that you followed their example config setup, let\u0026rsquo;s go into making posts! Your content directory should have a posts directory within it. If it doesn\u0026rsquo;t, go create that.\nEnsure that in your hugo.toml file, you have this:\n[[menu.main]]\rname = \u0026#34;Blog\u0026#34;\rweight = 1\rurl = \u0026#34;posts/\u0026#34;\r[[menu.main]]\rname = \u0026#34;About\u0026#34;\rweight = 2\rurl = \u0026#34;/about/\u0026#34; I changed mine around a bit, because I just wanted one singular about page. For now, run the following lines:\nhugo new \\content\\posts\\hello-world.md\rhugo new \\content\\about.md You\u0026rsquo;ll see that the files already come populated with some stuff in between some +++. These are essentially metadata that the theme utilizes to showcase and tag your posts. The prepopulated stuff can be found in the archetypes folder. Kudos to you if you explored a bit and found that out. It uses some regex to properly capitalize and figure out what to title your article. Fill the created files out to your desire.\nNow, run\nhugo server and navigate to the localhost link that it gives you! Ta-da! Your website it almost done! If you want to deploy it with GitHub pages like I have, go through the following steps:\nPush all of your changes and note which branch you used hugo version to see which version of hugo was installed Follow the instructions here Navigate to your website, and there you go! It\u0026rsquo;s all done. Final Thoughts #\rI hope this helped you out! If you have any questions, feel free to reach out to me!\n","date":"17 July 2024","externalUrl":null,"permalink":"/posts/first-blog-post/first-blog-post/","section":"Posts","summary":"or, a first timer\u0026rsquo;s journey messing with static sites.","title":"Pilot","type":"posts"},{"content":"\r$$2+2=9$$\nHi! My name is Kushal Vajrala. I\u0026rsquo;m a recent grad of UT Austin with a specialization in Software Engineering. Currently, I work at as a Data Science Analyst. This website is a place for me to document my learning journey in all of my interests. Hope you enjoy!\n","date":"17 July 2024","externalUrl":null,"permalink":"/about/","section":"Kushal Vajrala","summary":"$$2+2=9$$","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]